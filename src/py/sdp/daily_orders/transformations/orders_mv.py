from pyspark import pipelines as dp
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType
from pathlib import Path


# Define explicit schema for CSV reading (best practice for streaming)
orders_schema = StructType([
    StructField("order_id", StringType(), False),
    StructField("order_item", StringType(), False),
    StructField("price", FloatType(), False),
    StructField("items_ordered", IntegerType(), False),
    StructField("status", StringType(), False),
    StructField("state", StringType(), False),
    StructField("date_ordered", DateType(), False)
])


@dp.materialized_view(
    name="orders_mv",
    comment="Materialized view that reads order data from CSV files using Spark streaming ingestion."
)
def orders_mv() -> DataFrame:
    """
    Materialized view that reads order data from CSV files in the data/ directory.

    This demonstrates SDP's batch ingestion pattern, reading CSV files that were
    generated by the generate_csv_data.py script. The CSV files serve as the source
    of truth for order data, showcasing a realistic data ingestion pattern.

    Returns:
        DataFrame: DataFrame containing order items from CSV files.
    """
    spark = SparkSession.active()

    # Path to CSV data directory (relative to transformations directory)
    data_dir = Path(__file__).parent.parent / "data"

    # Read CSV files with explicit schema
    # Note: Using read (batch) instead of readStream for SDP compatibility
    # SDP materialized views work with batch DataFrames
    orders_df = (spark.read
                 .schema(orders_schema)
                 .option("header", "true")
                 .csv(str(data_dir)))

    return orders_df

